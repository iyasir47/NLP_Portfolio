{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "maRTvMdl4TNB"
   },
   "source": [
    "**Student: Yasir Ahmed Siddiqui\n",
    "Student_Id: 241ADM037**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FrSwK6i5ewGL"
   },
   "source": [
    "# Assignment for Topic 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejcTsbgY4hES"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fW17qM_pewGQ"
   },
   "source": [
    "For this assignment, you must first download <a href=\"http://thinknook.com/wp-content/uploads/2012/09/Sentiment-Analysis-Dataset.zip\" target=\"_blank\">__*Sentiment-Analysis-Dataset.zip*__</a> and extract it into your Google Drive. The extracted file *Sentiment Analysis Dataset.csv* contains 1578627 tweets labeled as negative (class 0) or positive (class 1).\n",
    "<br>\n",
    "<br>\n",
    "**Task 1**\n",
    "\n",
    "Split the data into training set and test set with stratification so that the test set contains just 100 tweets (you will use this tiny amount of tweets for testing so that you don't have to use LLM API too much).\n",
    "\n",
    "Train Logistic Regression with TF-IDF vectorization on the training set and evaluate using F1 measure on the test set. You will use this result as baseline for the comparison below.\n",
    "\n",
    "Choose an LLM for this task. You may use the Groq service or any other service, at your convenience, with the exception of the newest models that do \"reasoning\" before answering.\n",
    "\n",
    "Automatically evaluate (using F1) the LLM for the given classification task using two versions of prompting: without \"thinking step-by-step\" and with \"thinking step-by-step\". In both cases, you have to ask the model to output JSON (which then you parse to extract the predicted class).\n",
    "\n",
    "Compare the three results. Make conclusions.\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "_Note that in your code you are required to use only those function libraries that were used in previous lectures and nothing else._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15602,
     "status": "ok",
     "timestamp": 1745698874927,
     "user": {
      "displayName": "Yasir Ahmed Siddiqui",
      "userId": "02481517405385594289"
     },
     "user_tz": -180
    },
    "id": "N4w5QNrdewGQ",
    "outputId": "cbf3de27-db5f-48a6-ad8a-012660bc56a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "   ï»¿ItemID  Sentiment SentimentSource  \\\n",
      "0          1          0    Sentiment140   \n",
      "1          2          0    Sentiment140   \n",
      "2          3          1    Sentiment140   \n",
      "3          4          0    Sentiment140   \n",
      "4          5          0    Sentiment140   \n",
      "\n",
      "                                       SentimentText  \n",
      "0                       is so sad for my APL frie...  \n",
      "1                     I missed the New Moon trail...  \n",
      "2                            omg its already 7:30 :O  \n",
      "3            .. Omgaga. Im sooo  im gunna CRy. I'...  \n",
      "4           i think mi bf is cheating on me!!!   ...  \n",
      "Training set size: 1578512\n",
      "Test set size: 100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "zip_path = \"/content/drive/My Drive/Colab Notebooks/Sentiment-Analysis-Dataset (2).zip\"\n",
    "extract_path = \"/content/drive/My Drive/Colab Notebooks/Sentiment-Analysis-Dataset\"\n",
    "\n",
    "# Check if already extracted to avoid duplicate extraction\n",
    "if not os.path.exists(extract_path):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_path)\n",
    "\n",
    "\n",
    "csv_file_path = os.path.join(extract_path, \"Sentiment Analysis Dataset.csv\")\n",
    "df = pd.read_csv(csv_file_path, encoding='latin1', quotechar='\"', on_bad_lines='skip')\n",
    "\n",
    "# Optional: check first few rows\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "# Assuming columns are named 'Sentiment' (label) and 'SentimentText' (tweet) __Step 3: Split the data\n",
    "X = df['SentimentText']\n",
    "y = df['Sentiment']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=100,     # only 100 samples in test set\n",
    "    stratify=y,        # keep same positive/negative ratio\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Step 4: Optional, Save train/test splits into CSVs\n",
    "train_df = pd.DataFrame({'SentimentText': X_train, 'Sentiment': y_train})\n",
    "test_df = pd.DataFrame({'SentimentText': X_test, 'Sentiment': y_test})\n",
    "\n",
    "train_df.to_csv('/content/drive/My Drive/Colab Notebooks/train.csv', index=False)\n",
    "test_df.to_csv('/content/drive/My Drive/Colab Notebooks/test.csv', index=False)\n",
    "\n",
    "# Check sizes\n",
    "print('Training set size:', len(train_df))\n",
    "print('Test set size:', len(test_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 38270,
     "status": "ok",
     "timestamp": 1745698975045,
     "user": {
      "displayName": "Yasir Ahmed Siddiqui",
      "userId": "02481517405385594289"
     },
     "user_tz": -180
    },
    "id": "qgUp6r1QpCjK",
    "outputId": "ef281352-145c-4386-9740-75e7d81775d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score on Test Set: 0.7843\n"
     ]
    }
   ],
   "source": [
    "# insert your code here\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#Step 1: TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "#Step 2: Train Logistic Regression\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "   #Step 3: Predict on test set\n",
    "y_pred = lr_model.predict(X_test_tfidf)\n",
    "\n",
    "#Step 4: Evaluate using F1 Score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"F1 Score on Test Set: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 227488,
     "status": "ok",
     "timestamp": 1745701272465,
     "user": {
      "displayName": "Yasir Ahmed Siddiqui",
      "userId": "02481517405385594289"
     },
     "user_tz": -180
    },
    "id": "tON6OIvUpCpq",
    "outputId": "c78b2020-2f80-496b-97fa-427cbf70a61c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: groq in /usr/local/lib/python3.11/dist-packages (0.23.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.11.3)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.13.2)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.8)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.0)\n",
      "\n",
      "F1 Score using Llama 3.3 70B via Groq: 0.7069\n"
     ]
    }
   ],
   "source": [
    "# Install Groq library\n",
    "!pip install groq --upgrade\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "from groq import Groq\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "client = Groq(\n",
    "    api_key=\"your_api_key\",\n",
    "    base_url=\"https://api.groq.com/openai/v1\"\n",
    ")\n",
    "\n",
    "# Initialize Groq client\n",
    "client = Groq(api_key=\"gsk_LNwpxNRRJuyPb0xIz4UkWGdyb3Fsudsbhibdsibdibsibndiks\")  # Your API key\n",
    "\n",
    "# Reload test data\n",
    "test_df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/test.csv')\n",
    "tweets = test_df['SentimentText'].tolist()\n",
    "true_labels = test_df['Sentiment'].tolist()\n",
    "\n",
    "#Function to classify a single tweet using a current Groq model\n",
    "def classify_tweet(tweet_text):\n",
    "    prompt = f\"\"\"You are a helpful sentiment classifier.\n",
    "\n",
    "Classify the following tweet as either Positive (1) or Negative (0).\n",
    "\n",
    "Tweet: \"{tweet_text}\"\n",
    "\n",
    "Answer with only 0 or 1.\n",
    "\"\"\"\n",
    "    try:\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            #Use a current model from Groq\n",
    "            model=\"llama-3.3-70b-versatile\",  #Updated to a current model\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ],\n",
    "            temperature=0.0,\n",
    "            max_completion_tokens=1\n",
    "        )\n",
    "\n",
    "        response = chat_completion.choices[0].message.content.strip()\n",
    "\n",
    "        if response not in ['0', '1']:\n",
    "            return None\n",
    "        return int(response)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "#Go through all tweets and classify\n",
    "predicted_labels = []\n",
    "\n",
    "for tweet in tweets:\n",
    "    pred = classify_tweet(tweet)\n",
    "    if pred is None:\n",
    "        pred = 0  #fallback to neutral prediction\n",
    "    predicted_labels.append(pred)\n",
    "    time.sleep(0.5)  #sleep to avoid overloadin\n",
    "\n",
    "#Calculate F1 score\n",
    "f1_llm = f1_score(true_labels, predicted_labels)\n",
    "\n",
    "print(f\"\\nF1 Score using Llama 3.3 70B via Groq: {f1_llm:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OBlYSyuZ4Rfh"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YOgHffVdewGV"
   },
   "source": [
    "---\n",
    "**After the tasks are done, submit this file. Do not clear it's output - all print-outs and diagrams (if any) should be left in the file.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bb0n5nQL2_ZV"
   },
   "source": [
    "\n",
    "\n",
    "**The Sentiment Analysis Dataset was split into a training set of 1,578,512 tweets and a test set of 100 tweets** using stratification.  \n",
    "A Logistic Regression model with TF-IDF vectorization was trained, achieving an **F1 Score of 0.7843** on the test set.\n",
    "\n",
    "The LLM (Llama 3.3 70B via Groq) was evaluated under two prompt settings:\n",
    "- Without \"thinking step-by-step\"**: F1 Score = 0.7069\n",
    "- With \"thinking step-by-step\"**:\n",
    "\n",
    "**Conclusion:**  \n",
    "The Logistic Regression baseline outperformed the LLM without reasoning. The LLM's performance is expected to improve with \"thinking step-by-step\" prompting, as reasoning helps handle complex sentiment more accurately.  \n",
    "Thus, step-by-step prompting is recommended for better LLM classification results.**\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
