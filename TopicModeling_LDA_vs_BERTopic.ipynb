{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8b19ZYZVFC2R"
   },
   "source": [
    "**Yasir Ahmed Siddiqui _241ADM037**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aMAxOqheE1np"
   },
   "source": [
    "**Assignment for Topic **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IgpUtw-vEy8h"
   },
   "source": [
    "In this assignment, you will work with the given brown-small corpus (which is a small version of the Brown corpus).\n",
    "\n",
    "Task 1\n",
    "\n",
    "Perform topic modeling using the LDA with 10 topics. Name your topics using an LLM. (No plotting of any kind is required.)\n",
    "\n",
    "Task 2\n",
    "\n",
    "Perform topic modeling using the BERTopic. Name your topics using an LLM. Plot one scatterplot visualizing your results.\n",
    "\n",
    "Now compare the extracted topics between Task 1 and Task 2 and make conclusions.\n",
    "\n",
    "\n",
    "Note that in your code you are required to use only those function libraries that were used in previous lectures and nothing else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 799
    },
    "executionInfo": {
     "elapsed": 30725,
     "status": "ok",
     "timestamp": 1746389038540,
     "user": {
      "displayName": "Yasir Ahmed Siddiqui",
      "userId": "02481517405385594289"
     },
     "user_tz": -180
    },
    "id": "S299Gkcf5FaN",
    "outputId": "27f9bde3-449c-4f5a-b50f-511b312033f5"
   },
   "outputs": [],
   "source": [
    "!pip install numpy==1.24.4 scipy==1.10.1 gensim==4.3.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43609,
     "status": "ok",
     "timestamp": 1746389097852,
     "user": {
      "displayName": "Yasir Ahmed Siddiqui",
      "userId": "02481517405385594289"
     },
     "user_tz": -180
    },
    "id": "wU9lR3ou5W3V",
    "outputId": "727606d7-159f-428e-f984-b55312c2cd6f"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import gensim\n",
    "\n",
    "\n",
    "\n",
    "nltk.download('brown')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# custom domain-specific stopwords\n",
    "custom_stopwords = [\n",
    "    'said', 'says', 'also', 'would', 'whose', 'well', 'non',\n",
    "    'may', 'go', 'goes', 'going', 'went', 'like', 'cannot',\n",
    "    'however', 'u'\n",
    "]\n",
    "stopwords.extend(custom_stopwords)\n",
    "\n",
    "# Define the split and normalize function\n",
    "def split_and_normalize(docs, stop):\n",
    "    lists_of_words = []\n",
    "    for doc in docs:\n",
    "        # Split on non-word characters\n",
    "        words = re.split(r'\\W+', doc)\n",
    "        # Lowercase and remove stopwords + non-alpha words\n",
    "        words = [word.lower() for word in words if word.isalpha() and word.lower() not in stop]\n",
    "        lists_of_words.append(words)\n",
    "    return lists_of_words\n",
    "\n",
    "# Load the Brown corpus (e.g., 'news' category)\n",
    "from nltk.corpus import brown\n",
    "docs = [' '.join(sent) for sent in brown.sents(categories='news')]\n",
    "\n",
    "# Process the documents\n",
    "cleaned_docs = split_and_normalize(docs, stopwords)\n",
    "\n",
    "# Create a dictionary mapping for all words\n",
    "dictionary = gensim.corpora.Dictionary(cleaned_docs)\n",
    "\n",
    "# Filter out extreme tokens\n",
    "dictionary.filter_extremes(no_below=2, no_above=0.5)\n",
    "\n",
    "print(\"Number of unique words after filtering:\", len(dictionary))\n",
    "\n",
    "# Create Bag-of-Words representation for each document\n",
    "corpus_bow = [dictionary.doc2bow(doc) for doc in cleaned_docs]\n",
    "\n",
    "# Train the LDA model with 10 topics\n",
    "lda_model = gensim.models.LdaMulticore(\n",
    "    corpus=corpus_bow,\n",
    "    id2word=dictionary,\n",
    "    num_topics=10,       # Change this to desired number of topics\n",
    "    passes=20,\n",
    "    iterations=500,\n",
    "    workers=2,           # Adjust based on CPU\n",
    "    chunksize=1000,\n",
    "    eval_every=None,\n",
    "    random_state=1\n",
    ")\n",
    "\n",
    "#Display the topics\n",
    "for i in range(lda_model.num_topics):\n",
    "    terms = [word for word, prob in lda_model.show_topic(i, topn=10)]\n",
    "    print(f\"Topic #{i}: {', '.join(terms)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1552,
     "status": "ok",
     "timestamp": 1746390982473,
     "user": {
      "displayName": "Yasir Ahmed Siddiqui",
      "userId": "02481517405385594289"
     },
     "user_tz": -180
    },
    "id": "wew9m2rt9fav",
    "outputId": "f1a59f7e-1792-4274-96a8-15f5db8e3055"
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "api_key = getpass.getpass(\"gsk_CsCRXVkPUwkP9q8MCd\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1746390667982,
     "user": {
      "displayName": "Yasir Ahmed Siddiqui",
      "userId": "02481517405385594289"
     },
     "user_tz": -180
    },
    "id": "bTcmaky59qZO",
    "outputId": "bbf05ab7-ab66-4dce-a3b4-f404a92ffd66"
   },
   "outputs": [],
   "source": [
    "lda_model.print_topics(num_words=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3338,
     "status": "ok",
     "timestamp": 1746391518993,
     "user": {
      "displayName": "Yasir Ahmed Siddiqui",
      "userId": "02481517405385594289"
     },
     "user_tz": -180
    },
    "id": "YNxZSqr5C7CB",
    "outputId": "fc16862c-913e-4bad-f014-784ffb562243"
   },
   "outputs": [],
   "source": [
    "!pip install groq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27568,
     "status": "ok",
     "timestamp": 1746391902469,
     "user": {
      "displayName": "Yasir Ahmed Siddiqui",
      "userId": "02481517405385594289"
     },
     "user_tz": -180
    },
    "id": "XyJLgD8xEF3B",
    "outputId": "215d2adf-e1ec-4910-efa2-961f13b9507e"
   },
   "outputs": [],
   "source": [
    "#Format the topics and call Groq's LLaMA model to name them using groq SDK\n",
    "from groq import Groq\n",
    "import getpass  # For secure API key input\n",
    "\n",
    "# Securely enter the API key\n",
    "api_key = getpass.getpass(\"gsk_CsCRXVkPUwk\").strip()\n",
    "\n",
    "# === Validate API key before continuing ===\n",
    "if not api_key or not api_key.startswith(\"gsk_\"):\n",
    "    sys.exit(\" Invalid or missing API key.  enter a valid key that starts with 'gsk_'.\")\n",
    "\n",
    "# === Initialize Groq client ===\n",
    "try:\n",
    "    client = Groq(api_key=api_key)\n",
    "except Exception as e:\n",
    "    sys.exit(f\" Failed to initialize Groq client: {e}\")\n",
    "\n",
    "# === Extract topics from LDA ===\n",
    "topics_keywords = lda_model.show_topics(num_topics=10, num_words=10, formatted=False)\n",
    "formatted_topics = [\n",
    "    f\"Topic {i}: {', '.join(word for word, _ in keywords)}\"\n",
    "    for i, keywords in topics_keywords\n",
    "]\n",
    "prompt_body = \"\\n\".join(formatted_topics)\n",
    "\n",
    "# === Prepare prompts ===\n",
    "system_prompt = \"You are an expert in topic modeling and academic writing.\"\n",
    "user_prompt = (\n",
    "    \"Below are 10 topics extracted from a topic model (LDA), each with its top 10 keywords:\\n\\n\"\n",
    "    f\"{prompt_body}\\n\\n\"\n",
    "    \"Please provide a clear, concise, and academic-style name for each topic.\\n\"\n",
    "    \"Return the results in the format: Topic X – [Descriptive Name]\\n\"\n",
    ")\n",
    "\n",
    "# === Query Groq LLaMA-3 model ===\n",
    "try:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama3-8b-8192\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0.5,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    output = response.choices[0].message.content\n",
    "except Exception as e:\n",
    "    sys.exit(f\" Error while querying Groq API: {e}\")\n",
    "\n",
    "# === Output ===\n",
    "print(\" Named Topics:\\n\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yh97IgpbEsyJ"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fcUOKtVjJJvr",
    "outputId": "fb5ba0a0-9a13-4d18-a1dd-391ad40f301a"
   },
   "outputs": [],
   "source": [
    "# Install or upgrade necessary libraries\n",
    "!pip install --upgrade numpy\n",
    "!pip install --upgrade bertopic scikit-learn umap-learn\n",
    "\n",
    "#Restart the runtime after the installation (if using Jupyter/Colab)\n",
    "import os\n",
    "os._exit(00)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "3951319c60f64b749b0f497466a85079",
      "f79307505b4042058f893d1123e660ee",
      "c1d35971b3c547a79a42b03d312c0e4e",
      "86317ba9f928472c920dc6bb631e77cc",
      "43cee2fde94141b4b0ed14f961cd5d5c",
      "1c0b87241a5b4209aa21ccc0d20b89dd",
      "626dc6b9090b40b5b56d5ba09792e5ce",
      "10105abc239043b9b579d0098ee1a55d",
      "14ff1c67580e4791b198a11ebd0e45ab",
      "cdb05c83b264410c9e57ca84c1fbdfdc",
      "78144a7161394782b1e40fe5a9e0c664"
     ]
    },
    "executionInfo": {
     "elapsed": 117946,
     "status": "ok",
     "timestamp": 1746396352837,
     "user": {
      "displayName": "Yasir Ahmed Siddiqui",
      "userId": "02481517405385594289"
     },
     "user_tz": -180
    },
    "id": "LmlHNI_PGK6K",
    "outputId": "7906d5fb-c515-4354-bd21-4dcb25b4713d"
   },
   "outputs": [],
   "source": [
    "!pip install -q bertopic groq nltk scikit-learn umap-learn\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from bertopic import BERTopic\n",
    "from groq import Groq\n",
    "import getpass\n",
    "import sys\n",
    "\n",
    "nltk.download('brown')\n",
    "nltk.download('punkt')\n",
    "\n",
    "#Prepare text data\n",
    "docs = [' '.join(sent) for sent in brown.sents(categories='news')]\n",
    "\n",
    "#Train BERTopic model\n",
    "topic_model = BERTopic(language=\"english\", verbose=True)\n",
    "topics, probs = topic_model.fit_transform(docs)\n",
    "\n",
    "#Extract topic keywords\n",
    "topic_info = topic_model.get_topic_info()\n",
    "topic_keywords = topic_info[topic_info.Topic != -1][['Topic', 'Representation']]\n",
    "\n",
    "#Format topic descriptions\n",
    "formatted_topics = []\n",
    "for _, row in topic_keywords.iterrows():\n",
    "    topic_num = row['Topic']\n",
    "    keywords = row['Representation'][:10]  # Already a list\n",
    "    formatted_topics.append(f\"Topic {topic_num}: {', '.join(keywords)}\")\n",
    "\n",
    "prompt_body = \"\\n\".join(formatted_topics)\n",
    "\n",
    "#Step 4: Define prompts\n",
    "system_prompt = \"You are an expert in topic modeling and academic writing.\"\n",
    "user_prompt = (\n",
    "    \"Below are topics extracted using BERTopic, each with its top 10 keywords:\\n\\n\"\n",
    "    f\"{prompt_body}\\n\\n\"\n",
    "    \"Please provide a clear, concise, and academic-style name for each topic.\\n\"\n",
    "    \"Return the results in the format: Topic X – [Descriptive Name]\\n\"\n",
    ")\n",
    "\n",
    "#API key input for Groq\n",
    "api_key = getpass.getpass(\"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\").strip()\n",
    "if not api_key or not api_key.startswith(\"gsk_\"):\n",
    "    sys.exit(\"Invalid or missing API key. Please enter a valid key that starts with 'gsk_'.\")\n",
    "\n",
    "#Query Groq to name topics\n",
    "try:\n",
    "    client = Groq(api_key=api_key)\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama3-8b-8192\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0,\n",
    "        max_tokens=300\n",
    "    )\n",
    "    bertopic_named_output = response.choices[0].message.content\n",
    "except Exception as e:\n",
    "    sys.exit(f\"Error while querying Groq API: {e}\")\n",
    "\n",
    "#Output BERTopic-named topics\n",
    "print(\"Named Topics from BERTopic:\\n\")\n",
    "print(bertopic_named_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 685
    },
    "executionInfo": {
     "elapsed": 286,
     "status": "ok",
     "timestamp": 1746396364550,
     "user": {
      "displayName": "Yasir Ahmed Siddiqui",
      "userId": "02481517405385594289"
     },
     "user_tz": -180
    },
    "id": "_-8g1nx_R_fF",
    "outputId": "3e8c358d-7370-4c8c-bb90-9967de105f47"
   },
   "outputs": [],
   "source": [
    "#Visualize only the top 10 topics using a scatter plot\n",
    "print(\"Generating scatter plot for Top 10 Topics...\")\n",
    "\n",
    "#Get the top 10 topic IDs (excluding -1 which is 'outlier')\n",
    "top_topics = topic_model.get_topic_info().head(11)['Topic'].tolist()\n",
    "top_topics = [t for t in top_topics if t != -1][:10]  # Ensure only 10 topics and no outliers\n",
    "\n",
    "#Generate scatter plot for selected topics only\n",
    "fig = topic_model.visualize_topics(top_n_topics=10)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8jrkYqs0PZrS"
   },
   "source": [
    "| Feature                | Task 1: LDA                          | Task 2: BERTopic                           |\n",
    "| ---------------------- | ------------------------------------ | ------------------------------------------ |\n",
    "| **Algorithm**          | Latent Dirichlet Allocation (Gensim) | BERT + Clustering + UMAP (BERTopic)        |\n",
    "| **Embeddings**         | Bag-of-Words based                   | Contextual Embeddings (Transformer-based)  |\n",
    "| **Topic Coherence**    | Often generic (word co-occurrence)   | More specific and human-readable topics    |\n",
    "| **Custom Topic Names** | Named via LLaMA using keywords       | Named via LLaMA using rich representations |\n",
    "| **Topic Overlap**      | Can be overlapping                   | More distinct and refined topics           |\n",
    "| **Visualization**      | Not inherently visual                | Built-in UMAP scatter         |\n",
    "| **Best For**           | Simpler datasets                     | Rich, contextual datasets (e.g., news)     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RuH9u5GiPpl0"
   },
   "source": [
    "**Conslusion:** evaluating LDA (Task 1) against BERTopic (Task 2), several key differences emerge. BERTopic leverages advanced language embeddings and dimensionality reduction, enabling it to identify topics that are more contextually nuanced and semantically rich than those produced by traditional LDA. Studies have shown that BERTopic generates more coherent and well-separated topic clusters, as visualized through scatterplots and other built-in tools, which aids in intuitive interpretation-an area where LDA typically lacks built-in support.\n",
    "\n",
    "Moreover, integrating large language models with BERTopic streamlines the process of naming and describing topics, resulting in more precise and human-readable labels. In contrast, LDA often requires manual intervention and extensive preprocessing for topic interpretation. While LDA remains a valuable tool for large-scale text analysis, especially when computational resources are limited, BERTopic stands out for its ability to capture deeper semantic relationships and provide enhanced interpretability and actionable insights from complex text datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7vei6BtFExVd"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMKPoCZ6AWWEzRcEqtZxXSw",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
