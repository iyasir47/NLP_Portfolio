{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FrSwK6i5ewGL"
   },
   "source": [
    "# Assignment for Topic 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fW17qM_pewGQ"
   },
   "source": [
    "For this assignment, you must first download <a href=\"http://thinknook.com/wp-content/uploads/2012/09/Sentiment-Analysis-Dataset.zip\" target=\"_blank\">__*Sentiment-Analysis-Dataset.zip*__</a> and extract it into your Google Drive. The extracted file *Sentiment Analysis Dataset.csv* contains 1578627 tweets labeled as negative (class 0) or positive (class 1).\n",
    "<br>\n",
    "<br>\n",
    "**Task 1**\n",
    "\n",
    "Load the first 50 000 tweets from the file and split the data 80:20 into training and test sets with stratification.\n",
    "\n",
    "Load the `sentence-transformers/all-mpnet-base-v2` model and use it to train (on the training set) and evaluate (on the test set) the following classifiers:\n",
    "* \"Classification without classifier\";\n",
    "* \"Zero-Shot Classification\" (try to find useful texts as your labels for embedding);\n",
    "* Supervised classification with Logistic Regression on top;\n",
    "* Supervised classification with Support Vector Machine on top.\n",
    "\n",
    "Now load the `cardiffnlp/twitter-roberta-base-sentiment-latest` and use it as yet another classifier to evaluate on the test set.\n",
    "\n",
    "Now train and evaluate the following two models:\n",
    "* Logistic Regression with TF-IDF vectorization;\n",
    "* Support Vector Machine with TF-IDF vectorization.\n",
    "\n",
    "Additional notes:\n",
    "* In total you will have 7 classifiers.\n",
    "* The evaluation should be done so that we can see at least the F1 measure for each separate class as well as its macro average. But you can also have other measures.\n",
    "\n",
    "Finally, write conclusions about all the results you got.\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "_Note that in your code you are required to use only those function libraries that were used in previous lectures and nothing else._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 491,
     "referenced_widgets": [
      "43c1e66c2e7445abbbda4eff3b951796",
      "d368822c72c34dc7938f9c65d8b81bb7",
      "cf342e7f392e424f89bc6402ccde2b44",
      "a6e9214d8adc40b9ae3310286b866351",
      "069266aa1302473bbe74e5d7fc73b747",
      "2c83ccaa417e44c2a84ac3b896853ad7",
      "8737db3969dc4687b6af6d047aaaffa8",
      "29322eb96c0049b1afcd48756dd48050",
      "a844482f197444e0932e89e1b40b4095",
      "4f8b46feb8ac416ebfe0c4a7abdb8337",
      "ef1cdfa1efe1481bb97d89f0fd5e0cc0",
      "c08bb58fc35f44368be3d41e6c485edb",
      "dfe3b5d79060474cb354655893b0de8e",
      "38939f0f56514af996ec9ab40e1ff648",
      "4aa23254077b4ce4bbececa03b394964",
      "48ac2559fe5346cba4d8e712d8b10edf",
      "761f08c619b947ff8c222b5610b6b065",
      "468ea5ac07a6434d82632d56ccadd94b",
      "b7c1a6a2d9264255b65086b2ba607eab",
      "cebdff7c843e4e71bfced287dc810c99",
      "b87c050447bc45fc8966847c1e308c0e",
      "fcffd9b7a1e64a65a2670cedc073780a",
      "a383b166998b4134878c3f1db4feb4de",
      "7fd31fae40374edfa53ddba2bbdaf895",
      "7719812a78ac4a0bb3455f9472529536",
      "6d8f2141b5ce4aaaa1b2f8c4645d80e6",
      "77ae3f3dfc4842078906ba9c6292d3a1",
      "2f9c2a2f4d3c489c9abe95f4779a57fd",
      "a58d0018c42641e7b2c4420730ec04d5",
      "bbd5f4ae886242d7b70d5c222d655fa6",
      "850a65bacf8f4c7db1eeb2b7e5ac0577",
      "c3130f245b564ae1ba491a84b634cb3d",
      "cf5c46e781194c80ac9f2cf03c7275c9",
      "c49c93f1c7424b9896f528f6275840f7",
      "c497374f47da4f18ba910a593dc9b609",
      "782bbfc7c82b4661b1b24ceec141d6ce",
      "f3c09ded818e43e98306cb2283577f1b",
      "e0f3188a2bfb4f03910060399205f264",
      "258f7c611df84611abb24967e20b20df",
      "89d84aae15cb486a9f8aff65a2049b90",
      "1623a97b218845e48c67f2b755431a48",
      "994d063ff3f94623ae715e86b11ffa43",
      "be4e76cf6ab84d01829b1d470a52b7bc",
      "55afef19370046c4b1c29425367041ed",
      "049f5fe8ccbe4de3a967a0767d9cd293",
      "8338c9850da1430ba54238ca8aaf348e",
      "349cddb52a514621bb4700b8924b63f9",
      "f9163de0dce9407e85c642548b2f8133",
      "5b7da10e37014198b7f5c9fe297ba8e9",
      "04debf3ed8ed40af80b4d711d23d8fd4",
      "013d6142d2b14e068da6ebacc5b156f9",
      "17a7e6db69de49809d121432000fe5f4",
      "b7a054ec7beb4f478fb569607df48539",
      "adb8682df64e4fb38ddf078b9369e52a",
      "5f8bba46370f4baa86d36fd6aa8d94b0",
      "7512feeb871b4a5e9ab6f7ff718c07d3",
      "a8a46fb9af1c499f910236ed55aa0866",
      "7cfe3a9d8c4f48a383b7502c240cdd40",
      "4e743a681cb44d5098d15172e1ff021c",
      "4214360888964188960d4e083fffd3aa",
      "a28178dc7a444c30a288cc11137ad3b1",
      "7af28f0e551e4e60b504376c9f10ca4b",
      "f57d43b3d84041d48fd1fc080afac11e",
      "debdb0e0a41d4dc194c3ee570b0ffb17",
      "d559516d8ac9417c93aea3fb9e621b21",
      "acf67f57ff8a42449eb5406231a579cb",
      "4ea5431c253a415fad3d699906bfc782",
      "9abd353596a24ce2959fd97627705d7d",
      "30c2c664a3cf44268c34f504db292c77",
      "7af85e9a0ff948daa855f23898ea18da",
      "c06f0554a2714ea8bd0a378dd1d39820",
      "a97cef19ccf34ae788ecd7a8dd43ffc2",
      "f24d2eba96d7489e98c94dd59e688df5",
      "0421c994647f4df69f2034fffc0d3e9b",
      "a2984bff6b594481b153afff7d1be3b3",
      "92ecf9151b8347b191a4b0b5861bdb26",
      "2b07860cadfb4be98f96950d7fc4727e",
      "6c04676c52664e158e5a5980dd447477",
      "d9e8569b97004bc6894ee22370d43ba2",
      "05b2ebe94e65485ba89b8019e0476778",
      "51116a05f6764350a222ff1eaf20b971",
      "96190f0f05b74ab6939d01e20ea5e3ff",
      "9edb7477e3f145a7a0b76e76e62d2822",
      "b735b12784244995a5ef08a41c25d9bf",
      "260dfecf39c54dd8893640d647527b64",
      "f31db57552d449bc8f8d0c0c7a988d21",
      "d866e95212e346afb02968ac333de71b",
      "366b54d28d584a30a742c88b45076056",
      "422c6c3e5ae44683b1dacdadcc6b4561",
      "b2a261e199cb40149b48b0793b156448",
      "16971070d021415080ff41fafd6ae678",
      "e70612489b1d4aefa0f81101a1c1ffbd",
      "e4e850b014054dfda73a8dd66c4c4489",
      "caa907a8baeb4e8c96a70e7259daf4e2",
      "302d8f2f292b4a9684682b82b9c3005b",
      "96c869d01a0746828b2e4b8999b22841",
      "96f2c6aaaf224f98ac72432e7d843769",
      "e00521590e4c47928582e6dceda9a5b4",
      "9e6b65feeb86410fa662c1bb65424a1d",
      "fee3b0c524a5487f904221477fdf8589",
      "31c7aafdcef24a9a8faf9195db8f8e27",
      "4bb1f46f89bc4484a37b2f31cf147994",
      "3356ce6df52647fb9351a46fee508333",
      "b137b2a3cd7043498215706ca7bf419f",
      "8aa26254e5f34075a49e157479a95ba2",
      "c9e1f25febb345fcbbbe73b218317349",
      "e694aba93b00480daa15a85e02c85f3b",
      "13060f0aea4b4ff58a0cc9cf69dd2c58",
      "f2ab7c802bbe4f8e81e26f6c647240b4",
      "507fe2dc24bc4d86b17684c7e97f39f6",
      "ff0960a2a8fa4482988be239451b3aa6",
      "3940deea5b2b4dcb85f2a80919f91bd2",
      "273cfc324a0d4632a9a6b3d263305583",
      "c6bc49a7b1bd464eaf7debf1dc44e2af",
      "3f716d739807483ea2e5268630a62754",
      "b7936695f14442caab40d642a1fc2e8e",
      "792420de51b24411a3aae3954c05d485",
      "652130beb2084d65b8581d83ee73e5d0",
      "9f956a9d5c664a05a7f1ff14ad7fd353",
      "8b52941df5a14e6286173ed936dea569",
      "57d71e3b2ab84bc99641351e9b4e1134"
     ]
    },
    "id": "60qfdM-r1xyC",
    "outputId": "348d2821-bded-429b-b5c4-ab5592918443"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from google.colab import drive\n",
    " \n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "file_path = \"/content/drive/My Drive/Colab Notebooks/Sentiment-Analysis-Dataset/Sentiment Analysis Dataset.csv\"\n",
    "\n",
    "# Handle CSV errors by skipping bad line\n",
    "df = pd.read_csv(file_path, encoding='latin-1', on_bad_lines='skip')\n",
    "\n",
    "# Ensure correct columns\n",
    "expected_columns = [\"ItemID\", \"Sentiment\", \"SentimentSource\", \"SentimentText\"]  #Example column names\n",
    "df.columns = expected_columns[:len(df.columns)]  #Adjust column names dynamically if needed\n",
    "\n",
    "# Select first 50,000 rows & required columns\n",
    "df = df.iloc[:50000, [1, 3]]  # Assuming column 1 = sentiment, column 3 = text\n",
    "df.columns = [\"label\", \"text\"]\n",
    "\n",
    "# Convert labels to integer type\n",
    "df[\"label\"] = df[\"label\"].astype(int)\n",
    "\n",
    "#Split dataset with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[\"text\"], df[\"label\"], test_size=0.2, stratify=df[\"label\"], random_state=42\n",
    ")\n",
    "\n",
    "#Load Sentence Transformer Model\n",
    "model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "#Encode text data\n",
    "X_train_embeddings = model.encode(X_train.tolist(), convert_to_numpy=True)\n",
    "X_test_embeddings = model.encode(X_test.tolist(), convert_to_numpy=True)\n",
    "\n",
    "#Classification without Classifier (Cosine Similarity)\n",
    "avg_positive_embedding = np.mean(X_train_embeddings[np.where(y_train == 1)], axis=0)\n",
    "avg_negative_embedding = np.mean(X_train_embeddings[np.where(y_train == 0)], axis=0)\n",
    "\n",
    "cosine_sim_pos = cosine_similarity(X_test_embeddings, avg_positive_embedding.reshape(1, -1))\n",
    "cosine_sim_neg = cosine_similarity(X_test_embeddings, avg_negative_embedding.reshape(1, -1))\n",
    "\n",
    "preds_without_classifier = (cosine_sim_pos > cosine_sim_neg).astype(int).flatten()\n",
    "print(\"✅ Accuracy (without classifier):\", accuracy_score(y_test, preds_without_classifier))\n",
    "\n",
    "#Logistic Regression Classifier\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train_embeddings, y_train)\n",
    "y_pred_lr = lr.predict(X_test_embeddings)\n",
    "print(\"✅ Accuracy (Logistic Regression):\", accuracy_score(y_test, y_pred_lr))\n",
    "\n",
    "#Support Vector Machine Classifier\n",
    "svm = SVC()\n",
    "svm.fit(X_train_embeddings, y_train)\n",
    "y_pred_svm = svm.predict(X_test_embeddings)\n",
    "print(\"✅ Accuracy (SVM):\", accuracy_score(y_test, y_pred_svm))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qgUp6r1QpCjK"
   },
   "outputs": [],
   "source": [
    "# insert your code here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import pipeline\n",
    "from google.colab import drive\n",
    "\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "\n",
    "file_path = \"/content/drive/My Drive/Colab Notebooks/Sentiment-Analysis-Dataset/Sentiment Analysis Dataset.csv\"\n",
    "\n",
    "#Handle CSV errors\n",
    "df = pd.read_csv(file_path, encoding='latin-1', on_bad_lines='skip')\n",
    "\n",
    "#Ensure correct columns\n",
    "expected_columns = [\"ItemID\", \"Sentiment\", \"SentimentSource\", \"SentimentText\"]\n",
    "df.columns = expected_columns[:len(df.columns)]  # Adjust dynamically if needed\n",
    "\n",
    "#Select first 50,000 rows & required columns\n",
    "df = df.iloc[:50000, [1, 3]]  # Assuming column 1 = sentiment, column 3 = text\n",
    "df.columns = [\"label\", \"text\"]\n",
    "df[\"label\"] = df[\"label\"].astype(int)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( #Split dataset\n",
    "    df[\"text\"], df[\"label\"], test_size=0.2, stratify=df[\"label\"], random_state=42\n",
    ")\n",
    "\n",
    "#Load Sentence Transformer Model\n",
    "model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "#Encode text data\n",
    "X_train_embeddings = model.encode(X_train.tolist(), convert_to_numpy=True)\n",
    "X_test_embeddings = model.encode(X_test.tolist(), convert_to_numpy=True)\n",
    "\n",
    "#Classification without Classifier (Cosine Similarity)\n",
    "avg_positive_embedding = np.mean(X_train_embeddings[np.where(y_train == 1)], axis=0)\n",
    "avg_negative_embedding = np.mean(X_train_embeddings[np.where(y_train == 0)], axis=0)\n",
    "\n",
    "cosine_sim_pos = cosine_similarity(X_test_embeddings, avg_positive_embedding.reshape(1, -1))\n",
    "cosine_sim_neg = cosine_similarity(X_test_embeddings, avg_negative_embedding.reshape(1, -1))\n",
    "\n",
    "preds_without_classifier = (cosine_sim_pos > cosine_sim_neg).astype(int).flatten()\n",
    "print(\"Accuracy (Cosine Similarity Classifier):\", accuracy_score(y_test, preds_without_classifier))\n",
    "print(classification_report(y_test, preds_without_classifier))\n",
    "\n",
    "#Logistic Regression Classifier (Sentence Transformer Embeddings)\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train_embeddings, y_train)\n",
    "y_pred_lr = lr.predict(X_test_embeddings)\n",
    "print(\"Accuracy (Logistic Regression - Sentence Transformer):\", accuracy_score(y_test, y_pred_lr))\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "\n",
    "#Support Vector Machine Classifier (Sentence Transformer Embeddings)\n",
    "svm = SVC()\n",
    "svm.fit(X_train_embeddings, y_train)\n",
    "y_pred_svm = svm.predict(X_test_embeddings)\n",
    "print(\"Accuracy (SVM - Sentence Transformer):\", accuracy_score(y_test, y_pred_svm))\n",
    "print(classification_report(y_test, y_pred_svm))\n",
    "\n",
    "#TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # Limit to 5000 features\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "#Logistic Regression Classifier (TF-IDF)\n",
    "lr_tfidf = LogisticRegression(max_iter=1000)\n",
    "lr_tfidf.fit(X_train_tfidf, y_train)\n",
    "y_pred_lr_tfidf = lr_tfidf.predict(X_test_tfidf)\n",
    "print(\" Accuracy (Logistic Regression - TF-IDF):\", accuracy_score(y_test, y_pred_lr_tfidf))\n",
    "print(classification_report(y_test, y_pred_lr_tfidf))\n",
    "\n",
    "#Support Vector Machine Classifier (TF-IDF)\n",
    "svm_tfidf = SVC()\n",
    "svm_tfidf.fit(X_train_tfidf, y_train)\n",
    "y_pred_svm_tfidf = svm_tfidf.predict(X_test_tfidf)\n",
    "print(\" Accuracy (SVM - TF-IDF):\", accuracy_score(y_test, y_pred_svm_tfidf))\n",
    "print(classification_report(y_test, y_pred_svm_tfidf))\n",
    "\n",
    "#Load Roberta Model (Pre-trained Sentiment Classifier)\n",
    "roberta_classifier = pipeline(\"text-classification\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "\n",
    "# Map Roberta outputs to 0 (negative) and 1 (positive)\n",
    "def roberta_predict(texts):\n",
    "    preds = roberta_classifier(texts, truncation=True)\n",
    "    return [1 if p['label'] == 'LABEL_2' else 0 for p in preds]  # Assuming LABEL_2 = Positive, LABEL_0 = Negative\n",
    "\n",
    "#Evaluate Roberta Model\n",
    "y_pred_roberta = roberta_predict(X_test.tolist())\n",
    "print(\"Accuracy (Roberta Classifier):\", accuracy_score(y_test, y_pred_roberta))\n",
    "print(classification_report(y_test, y_pred_roberta))\n",
    "\n",
    "#Summary of F1 Scores\n",
    "print(\"\\n **F1 Scores Summary:**\")\n",
    "print(f\"Cosine Similarity: {f1_score(y_test, preds_without_classifier, average='macro')}\")\n",
    "print(f\"Logistic Regression (ST): {f1_score(y_test, y_pred_lr, average='macro')}\")\n",
    "print(f\"SVM (ST): {f1_score(y_test, y_pred_svm, average='macro')}\")\n",
    "print(f\"Logistic Regression (TF-IDF): {f1_score(y_test, y_pred_lr_tfidf, average='macro')}\")\n",
    "print(f\"SVM (TF-IDF): {f1_score(y_test, y_pred_svm_tfidf, average='macro')}\")\n",
    "print(f\"Roberta Classifier: {f1_score(y_test, y_pred_roberta, average='macro')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tON6OIvUpCpq"
   },
   "outputs": [],
   "source": [
    "# insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QfSODxov6eLk"
   },
   "source": [
    "Conclusion\n",
    "RoBERTa achieved the highest accuracy due to pretraining on Twitter data.\n",
    "\n",
    "Sentence Transformer + Logistic Regression performed well, surpassing TF-IDF models.\n",
    "\n",
    "TF-IDF-based models were less effective, with Logistic Regression outperforming SVM.\n",
    "\n",
    "Cosine similarity (without classifier) was the weakest approach.\n",
    "\n",
    "Deep learning models significantly improved sentiment classification.\n",
    "\n",
    "RoBERTa is the best choice, while Logistic Regression + Sentence Transformer is a strong alternative for lower-resource settings.\n",
    "\n",
    "*   List item\n",
    "*   List item\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YOgHffVdewGV"
   },
   "source": [
    "---\n",
    "**After the tasks are done, submit this file. Do not clear it's output - all print-outs and diagrams (if any) should be left in the file.**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
